\begin{figure}[t]
  \centering
  \begin{tikzpicture}[
      >=stealth,
      node distance=1.2cm and 1.8cm,
      every node/.style={font=\small},
      neuron/.style={circle, draw, minimum size=10pt, inner sep=0pt},
      layerlabel/.style={font=\footnotesize\sffamily}
    ]

    % Input layer
    \foreach \i in {1,...,4} {
      \node[neuron, fill=gray!10] (I-\i) at (0, {-(\i-1)*1.0}) {};
    }
    \node[layerlabel, above=0.2cm of I-1, rug-red] {input layer};

    % Hidden layer
    \foreach \i in {1,...,5} {
      \node[neuron, fill=accentblue!10] (H-\i) at (2.3, {-(\i-1)*0.9+0.5}) {};
    }
    \node[layerlabel, above=0.2cm of H-1, rug-red] {hidden layer};

    % Output layer
    \foreach \i in {1,...,2} {
      \node[neuron, fill=red!10] (O-\i) at (4.6, {-(\i-1)*1.0-0.8}) {};
    }
    \node[layerlabel, above=0.2cm of O-1, rug-red] {output layer};

    % Connections: input to hidden
    \foreach \i in {1,...,4} {
      \foreach \j in {1,...,5} {
        \draw[->, thin] (I-\i) -- (H-\j);
      }
    }

    % Connections: hidden to output
    \foreach \i in {1,...,5} {
      \foreach \j in {1,...,2} {
        \draw[->, thin] (H-\i) -- (O-\j);
      }
    }

    % Labels for inputs/outputs
    \node[left=0.5cm of I-2] {$x_1,\dots,x_4$};
    \node[right=0.5cm of O-1] {$y_1,y_2$};

    % Legend arrow: gradient flow
    \begin{scope}[shift={(-0.2,-3.7)}]
      \draw[->, thin, gray] (1,0) -- (2,0);
      \node[font=\footnotesize, right=0.2cm] at (2,0)
        {information flow};
    \end{scope}
    
  \end{tikzpicture}
  \caption{Illustrative multi-layer perceptron with one hidden layer. Each neuron in a layer is fully connected to all neurons in the next layer, and information flows from left to right. This feed-forward architecture represents a basic building block of neural networks; deeper, convolutional, recurrent, or more complex models can all be viewed as extensions or refinements of this simple layered structure.}
  \label{fig::intro::mlp}
\end{figure}
