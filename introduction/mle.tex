\begin{figure}[t]
  \centering

  % Define the log-likelihood surface
  \pgfmathdeclarefunction{LLsurf}{2}{%
    % #1 = x, #2 = y
    \pgfmathparse{%
        2.2*exp(-((#1+1.5)^2 + (#2+1)^2))      % global peak
      + 1.4*exp(-((#1-2)^2   + (#2+2)^2)/1.3)  % local peak 1
      + 1.0*exp(-((#1-2)^2   + (#2-2)^2)/0.8)  % local peak 2
    }%
  }

  \begin{tikzpicture}
    \begin{axis}[
        hide axis,
        view={60}{35},
        domain=-4:4,
        y domain=-4:4,
        samples=38,
        scale=1.2,
        z buffer=sort,
        clip=false
    ]

      % Likelihood surface
      \addplot3[
        mesh,
        draw=gray!50,      % mesh line colour
        fill=gray!10,
        shader=interp
      ]
      {LLsurf(x,y)};

      % Mark the three optima
      \addplot3[
        only marks,
        mark=*,
        mark size=1.0pt,
        color=black
      ]
      coordinates {
        (-1.5,-1.0, {LLsurf(-1.5,-1.0)}) % global optimum
        ( 2.0, 2.0, {LLsurf( 2.0, 2.0)}) % local optimum 1
        %( 2.0,-2.0, {LLsurf( 2.0,-2.0)}) % local optimum 2
      };

      \node[font=\scriptsize,anchor=south,text=rug-red,xshift=26]
        at (axis cs:-1.5,-1.0,{LLsurf(-1.5,-1.0)+0.03}) {\faFlag~global optimum};      % global

      \node[font=\scriptsize,anchor=south,text=accentorange,xshift=24.5]
        at (axis cs:2.0,2.0,{LLsurf(2.0,2.0)+0.03}) {\faFlag~local optimum};          % local

      \node[font=\footnotesize,anchor=south,color=rug-red]
        at (axis cs:-1.5,-1.0,{LLsurf(-1.5,-1.0)+0.2})
        {};
      \node[font=\footnotesize,anchor=west,color=accentorange]
        at (axis cs:2.0,2.0,{LLsurf( 2.0, 2.0)+0.15})
        {};
      \node[font=\footnotesize,anchor=west]
        at (axis cs:2.0,-2.0,{LLsurf( 2.0,-2.0)+0.15})
        {};

      % ==================================================
      % Optimization paths
      % ==================================================

      % Path 1: starting far away and climbing to the LOCAL optimum at (2,2)
      \addplot3[
        domain=0:1,
        samples=30,
        variable=\t,
        very thick,
        color=accentorange
      ]
      (
        { 3.5 - 1.5*\t },           % x(t): from 3.5 to 2.0
        { -3.0 + 5.0*\t },          % y(t): from -3.0 to 2.0
        { LLsurf(3.5 - 1.5*\t, -3.0 + 5.0*\t) }
      );

      \node[font=\footnotesize,anchor=west, text=accentorange,yshift=-7,xshift=20]
        at (axis cs:3.5,-3.0,{LLsurf(3.5,-3.0)})
        {start (bad initials)};
      \node[font=\footnotesize,anchor=west, text=accentorange]
        at (axis cs:2.0,2.0,{LLsurf(2.0,2.0)+0.4})
        {};

      % Path 2: starting in another region and reaching the GLOBAL optimum
      \addplot3[
        domain=0:1,
        samples=30,
        variable=\t,
        very thick,
        color=rug-red
      ]
      (
        { -3.5 + 2.0*\t },          % x(t): from -3.5 to -1.5
        {  3.5 - 4.5*\t },          % y(t): from  3.5 to -1.0
        { LLsurf(-3.5 + 2.0*\t, 3.5 - 4.5*\t) }
      );

      \node[font=\footnotesize,anchor=east, text=rug-red,yshift=12,xshift=42]
        at (axis cs:-3.5,3.5,{LLsurf(-3.5,3.5)})
        {start (good initials)};
      \node[font=\footnotesize,anchor=east, text=rug-red]
        at (axis cs:-1.5,-1.0,{LLsurf(-1.5,-1.0)+0.4})
        {};

      % Mark the two starting points of the paths
      \addplot3[
        only marks,
        mark=*,
        mark size=1.0pt,
        color=black
      ]
      coordinates {
        (3.5,-3.0,{LLsurf(3.5,-3.0)})
      };

      \addplot3[
        only marks,
        mark=*,
        mark size=1.0pt,
        color=black
      ]
      coordinates {
        (-3.5,3.5,{LLsurf(-3.5,3.5)})
      };

    \end{axis}
  \end{tikzpicture}

    \caption{Illustrative log-likelihood surface with one global optimum and two local optima. 
    The colored curves show two optimization trajectories: one (red) that gets trapped in a local maximum and one (blue) that reaches the global maximum, depending on the starting point. 
    This highlights how, for complex and multi-modal likelihood functions, gradient-based MLE can be highly sensitive to initial values and may converge to suboptimal solutions, complicating reliable parameter estimation and uncertainty assessment. 
    Training neural networks relies on similar gradient-based optimization on equally rugged loss landscapes, so deep-learning approaches to likelihood-free inference can suffer from the same issues, even though they operate on a different objective.}
    \label{fig::intro::mle}
\end{figure}
